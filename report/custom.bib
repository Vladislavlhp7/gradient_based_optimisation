@ARTICLE{lecun98mnist,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{wilson2003generalinnefficiencybatchtraining,
    title = {The general inefficiency of batch training for gradient descent learning},
    journal = {Neural Networks},
    volume = {16},
    number = {10},
    pages = {1429-1451},
    year = {2003},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/S0893-6080(03)00138-2},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608003001382},
    author = {D.Randall Wilson and Tony R. Martinez},
    keywords = {Batch training, On-line training, Gradient descent, Backpropagation, Learning rate, Optimization, Stochastic approximation, Generalization},
    abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more ‘correct’ than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training—often orders of magnitude slower—especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy.}
}
@article{mishkin2017systematicconvolutionalneuralnetwork,
    doi = {10.1016/j.cviu.2017.05.007},
    url = {https://doi.org/10.1016%2Fj.cviu.2017.05.007},
    year = 2017,
    month = {aug},
    publisher = {Elsevier {BV}},
    volume = {161},
    pages = {11--19},
    author = {Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas},
    title = {Systematic evaluation of convolution neural network advances on the Imagenet},
    journal = {Computer Vision and Image Understanding}
}
@misc{keskar2017largebatch,
    title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
    author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
    year={2017},
    eprint={1609.04836},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{hoffer2017adagrad,
    author  = {John Duchi and Elad Hazan and Yoram Singer},
    title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2011},
    volume  = {12},
    number  = {61},
    pages   = {2121--2159},
    url     = {http://jmlr.org/papers/v12/duchi11a.html}
}
@incollection{ford2015numerical,
    title = {Chapter 12 - Linear System Applications},
    editor = {William Ford},
    booktitle = {Numerical Linear Algebra with Applications},
    publisher = {Academic Press},
    address = {Boston},
    pages = {241-262},
    year = {2015},
    isbn = {978-0-12-394435-1},
    doi = {https://doi.org/10.1016/B978-0-12-394435-1.00012-0},
    url = {https://www.sciencedirect.com/science/article/pii/B9780123944351000120},
    author = {William Ford},
    keywords = {Fourier series, Trigonometric functions, Infinite vector space, Infinite orthonormal basis, Cauchy-Schwarz inequality, Hilbert space, Piecewise continuously differentiable function, Square wave, Triangle wave, Sawtooth wave, Gibb’s phenomenon, Steady-state heat, Diffusion, Heat equation, Grid, Finite differences, Tridiagonal matrix, Sparse matrix, Thomas algorithm, Boundary conditions, Initial conditions, Least-squares, Vandermonde matrix, Interpolation, Extrapolation, Minimization, Partial derivatives, Normal equations, Overdetermined system, Underdetermined system, Absolute zero, Linear interpolation, Linear splines, Cubic splines, Cubic polynomial, Knots, Piecewise twice continuously differentiable function, Not-a-knot condition, Clamped cubic spline, Natural cubic spline}
}