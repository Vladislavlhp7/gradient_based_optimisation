@ARTICLE{lecun98mnist,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}
}
@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{wilson2003generalinnefficiencybatchtraining,
    title = {The general inefficiency of batch training for gradient descent learning},
    journal = {Neural Networks},
    volume = {16},
    number = {10},
    pages = {1429-1451},
    year = {2003},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/S0893-6080(03)00138-2},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608003001382},
    author = {D.Randall Wilson and Tony R. Martinez},
    keywords = {Batch training, On-line training, Gradient descent, Backpropagation, Learning rate, Optimization, Stochastic approximation, Generalization},
    abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more ‘correct’ than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training—often orders of magnitude slower—especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy.}
}
@article{mishkin2017systematicconvolutionalneuralnetwork,
    doi = {10.1016/j.cviu.2017.05.007},
    url = {https://doi.org/10.1016%2Fj.cviu.2017.05.007},
    year = 2017,
    month = {aug},
    publisher = {Elsevier {BV}},
    volume = {161},
    pages = {11--19},
    author = {Dmytro Mishkin and Nikolay Sergievskiy and Jiri Matas},
    title = {Systematic evaluation of convolution neural network advances on the Imagenet},
    journal = {Computer Vision and Image Understanding}
}
@misc{keskar2017largebatch,
    title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
    author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
    year={2017},
    eprint={1609.04836},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{smith2018dontdecay,
    title={Don't Decay the Learning Rate, Increase the Batch Size},
    author={Samuel L. Smith and Pieter-Jan Kindermans and Chris Ying and Quoc V. Le},
    year={2018},
    eprint={1711.00489},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@article{tieleman2012lecture,
    title={Lecture 6.5-rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude},
    author={Tieleman, Tijmen and Hinton, Geoffrey},
    journal={COURSERA: Neural Networks for Machine Learning},
    volume={4},
    pages={26--31},
    year={2012}
}
@misc{kingma2017adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2017},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
@misc{zeiler2012adadelta,
    title={ADADELTA: An Adaptive Learning Rate Method},
    author={Matthew D. Zeiler},
    year={2012},
    eprint={1212.5701},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@InProceedings{pmlr-v28-sutskever13,
    title = 	 {On the importance of initialization and momentum in deep learning},
    author = 	 {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
    booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
    pages = 	 {1139--1147},
    year = 	 {2013},
    editor = 	 {Dasgupta, Sanjoy and McAllester, David},
    volume = 	 {28},
    number =       {3},
    series = 	 {Proceedings of Machine Learning Research},
    address = 	 {Atlanta, Georgia, USA},
    month = 	 {17--19 Jun},
    publisher =    {PMLR},
    pdf = 	 {http://proceedings.mlr.press/v28/sutskever13.pdf},
    url = 	 {https://proceedings.mlr.press/v28/sutskever13.html},
    abstract = 	 {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.   }
}

@article{hoffer2017adagrad,
    author  = {John Duchi and Elad Hazan and Yoram Singer},
    title   = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    journal = {Journal of Machine Learning Research},
    year    = {2011},
    volume  = {12},
    number  = {61},
    pages   = {2121--2159},
    url     = {http://jmlr.org/papers/v12/duchi11a.html}
}
@incollection{ford2015numerical,
    title = {Chapter 12 - Linear System Applications},
    editor = {William Ford},
    booktitle = {Numerical Linear Algebra with Applications},
    publisher = {Academic Press},
    address = {Boston},
    pages = {241-262},
    year = {2015},
    isbn = {978-0-12-394435-1},
    doi = {https://doi.org/10.1016/B978-0-12-394435-1.00012-0},
    url = {https://www.sciencedirect.com/science/article/pii/B9780123944351000120},
    author = {William Ford},
    keywords = {Fourier series, Trigonometric functions, Infinite vector space, Infinite orthonormal basis, Cauchy-Schwarz inequality, Hilbert space, Piecewise continuously differentiable function, Square wave, Triangle wave, Sawtooth wave, Gibb’s phenomenon, Steady-state heat, Diffusion, Heat equation, Grid, Finite differences, Tridiagonal matrix, Sparse matrix, Thomas algorithm, Boundary conditions, Initial conditions, Least-squares, Vandermonde matrix, Interpolation, Extrapolation, Minimization, Partial derivatives, Normal equations, Overdetermined system, Underdetermined system, Absolute zero, Linear interpolation, Linear splines, Cubic splines, Cubic polynomial, Knots, Piecewise twice continuously differentiable function, Not-a-knot condition, Clamped cubic spline, Natural cubic spline}
}